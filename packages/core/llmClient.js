// llmClient.js
// –ú–æ–¥—É–ª—å –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –≤–Ω–µ—à–Ω–∏–º LLM API (kosmos-model)
const { readFileSync, existsSync } = require('fs');
const { join } = require('path');

// === –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ===
// –ü—É—Ç—å –∫ config.json
const configPath = join(process.cwd(), 'config.json');

/**
 * –ß—Ç–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏–∑ config.json (–¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏, –∫–∞–∂–¥—ã–π —Ä–∞–∑)
 * –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: config.json > process.env > –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
 * @returns {Object} { LLM_BASE_URL, LLM_API_KEY, LLM_MODEL }
 */
function getConfig() {
  let config = {};
  
  // –ß–∏—Ç–∞–µ–º config.json –∫–∞–∂–¥—ã–π —Ä–∞–∑
  if (existsSync(configPath)) {
    try {
      config = JSON.parse(readFileSync(configPath, 'utf-8'));
    } catch (error) {
      console.warn('‚ö†Ô∏è –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è config.json, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è:', error.message);
    }
  }
  
  return {
    LLM_BASE_URL: config.LLM_BASE_URL || process.env.LLM_BASE_URL || "http://localhost:3002/v1",
    LLM_API_KEY: process.env.LLM_API_KEY || "",
    LLM_MODEL: config.LLM_MODEL || process.env.LLM_MODEL || "FAST"
  };
}

// –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º –≥–µ—Ç—Ç–µ—Ä—ã –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
function getLLM_BASE_URL() {
  return getConfig().LLM_BASE_URL;
}

function getLLM_MODEL() {
  return getConfig().LLM_MODEL;
}

// === –¢–ò–ü–´ ===
/**
 * @typedef {Object} Message
 * @property {"system" | "user" | "assistant"} role
 * @property {string} content
 */

// === –û–°–ù–û–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø ===
/**
 * –í—ã–∑–æ–≤ LLM API –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞
 * @param {Message[]} messages - –ú–∞—Å—Å–∏–≤ —Å–æ–æ–±—â–µ–Ω–∏–π (system, user, assistant)
 * @param {string} model - –ò–º—è –º–æ–¥–µ–ª–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞)
 * @returns {Promise<string>} –¢–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏
 */
async function callLLM(messages, model = null) {
  // –ß–∏—Ç–∞–µ–º –∫–æ–Ω—Ñ–∏–≥ –∫–∞–∂–¥—ã–π —Ä–∞–∑
  const config = getConfig();
  const actualModel = model || config.LLM_MODEL;
  
  const headers = {
    "Content-Type": "application/json",
  };
  
  if (config.LLM_API_KEY) {
    headers["Authorization"] = `Bearer ${config.LLM_API_KEY}`;
  }

  try {
    console.log(`üì° –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ ${config.LLM_BASE_URL} (Model: ${actualModel})...`);
    console.log('Headers:', headers);
    console.log('Messages:', messages);
    console.log('Model:', actualModel);
    console.log('Temperature:', 0.3);
    console.log('Max Tokens:', 4096);
    
    const res = await fetch(`${config.LLM_BASE_URL}/chat/completions`, {
      method: "POST",
      headers,
      body: JSON.stringify({
        model: actualModel,
        messages,
        temperature: 0.0 // –ù–∞—Å—Ç—Ä–æ–π—Ç–µ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É –ø–æ–¥ –∑–∞–¥–∞—á–∏ (0.1 - –∫–æ–¥, 0.7 - –∫—Ä–µ–∞—Ç–∏–≤)
        // max_tokens: 4096, // –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ
      }),
    });

    if (!res.ok) {
      const errorText = await res.text();
      throw new Error(`LLM Error ${res.status}: ${errorText}`);
    }

    const json = await res.json();
    
    // –ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç–≤–µ—Ç–∞ (—Å—Ç–∞–Ω–¥–∞—Ä—Ç OpenAI)
    const response = json.choices?.[0]?.message?.content || "";
    
    if (!response) {
      throw new Error("–ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏ (—Å—Ç—Ä—É–∫—Ç—É—Ä–∞ JSON –º–æ–∂–µ—Ç –æ—Ç–ª–∏—á–∞—Ç—å—Å—è)");
    }

    return response;

  } catch (e) {
    console.error("‚ùå –û—à–∏–±–∫–∞ LLM:", e.message);
    throw e;
  }
}

// === –ü–†–û–í–ï–†–ö–ê –î–û–°–¢–£–ü–ù–û–°–¢–ò ===
/**
 * –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ LLM —Å–µ—Ä–≤–µ—Ä–∞
 * @param {number} timeout - –¢–∞–π–º–∞—É—Ç –≤ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥–∞—Ö (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 5000)
 * @returns {Promise<boolean>} true –µ—Å–ª–∏ —Å–µ—Ä–≤–µ—Ä –¥–æ—Å—Ç—É–ø–µ–Ω, false –∏–Ω–∞—á–µ
 */
async function checkLLMAvailability(timeout = 5000) {
  try {
    // –ß–∏—Ç–∞–µ–º –∫–æ–Ω—Ñ–∏–≥ –∫–∞–∂–¥—ã–π —Ä–∞–∑
    const config = getConfig();
    
    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), timeout);

    const headers = {
      "Content-Type": "application/json",
    };
    
    if (config.LLM_API_KEY) {
      headers["Authorization"] = `Bearer ${config.LLM_API_KEY}`;
    }

    // –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ç–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
    const res = await fetch(`${config.LLM_BASE_URL}/chat/completions`, {
      method: "POST",
      headers,
      signal: controller.signal,
      body: JSON.stringify({
        model: config.LLM_MODEL,
        messages: [{ role: "user", content: "test" }],
        max_tokens: 5, // –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
      }),
    });

    clearTimeout(timeoutId);

    // –ï—Å–ª–∏ –ø–æ–ª—É—á–∏–ª–∏ –æ—Ç–≤–µ—Ç (–¥–∞–∂–µ —Å –æ—à–∏–±–∫–æ–π), —Å–µ—Ä–≤–µ—Ä –¥–æ—Å—Ç—É–ø–µ–Ω
    return res.status === 200 || res.status === 400 || res.status === 401 || res.status === 404;
  } catch (error) {
    if (error.name === 'AbortError') {
      console.error(`‚è±Ô∏è LLM —Å–µ—Ä–≤–µ—Ä –Ω–µ –æ—Ç–≤–µ—Ç–∏–ª –≤ —Ç–µ—á–µ–Ω–∏–µ ${timeout}ms`);
    } else if (error.code === 'ECONNREFUSED' || error.code === 'ENOTFOUND') {
      console.error(`üîå LLM —Å–µ—Ä–≤–µ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: ${error.message}`);
    } else {
      console.error(`‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ LLM: ${error.message}`);
    }
    return false;
  }
}

// –≠–∫—Å–ø–æ—Ä—Ç –º–æ–¥—É–ª—è
// –î–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ LLM_BASE_URL –∏ LLM_MODEL –¥–æ—Å—Ç—É–ø–Ω—ã –∫–∞–∫ —Ñ—É–Ω–∫—Ü–∏–∏
// –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ getConfig() –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤—Å–µ—Ö –∑–Ω–∞—á–µ–Ω–∏–π —Å—Ä–∞–∑—É
module.exports = {
  callLLM,
  checkLLMAvailability,
  getConfig,
  getLLM_BASE_URL,
  getLLM_MODEL,
  // –î–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ - —Å–≤–æ–π—Å—Ç–≤–∞ –∫–∞–∫ —Ñ—É–Ω–∫—Ü–∏–∏
  // –í –∫–æ–¥–µ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ: LLM_BASE_URL() –≤–º–µ—Å—Ç–æ LLM_BASE_URL
  LLM_BASE_URL: getLLM_BASE_URL,
  LLM_MODEL: getLLM_MODEL
};

